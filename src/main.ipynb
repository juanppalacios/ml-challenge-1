{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Challenge #1 : Recommendation\n",
    "#### by JP Palacios\n",
    "This notebook details the process of implementing an auto-complete feature for citizen science checklist submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook will cover several phases of implementing an auto-complete feature: configuration, input, pre-processing, test case configuration, recommender algorithm, cross-validation, and output phases.\n",
    "\n",
    "## Configuration\n",
    "Before we begin, we have some file and library dependencies to sort out.\n",
    "This feature heavily relies on the `numpy` and `numba` libraries for data array operations and faster performance, respectively.\n",
    "The following python code imports all the necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot be imported, run as script!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba           import jit, cuda\n",
    "from rich.progress   import track\n",
    "\n",
    "# import custom scripts\n",
    "from common          import *\n",
    "from main            import *\n",
    "\n",
    "# note: added this to suppress numba deprecation warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning\n",
    "This notebook provides the user with the ability to tune hyper-parameters to test the effects of parameter combination on the output score.\n",
    "Our approach included three pre-processing methods, three distance metric functions, and a range of k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter lists\n",
    "all_preprocess_methods = ['normalizing', 'logarithms', 'clipping']\n",
    "selected_preprocess_method = all_preprocess_methods[1]\n",
    "\n",
    "metrics  = ['euclidean', 'cosine', 'jaccard']\n",
    "k_values = [20, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In Training & Testing Data Sets\n",
    "Now, we can read in our training and testing sets using our custom `read_input` function found in the `common.py` script.\n",
    "This function returns a dictionary with an `numpy` array and its dimensions as individual keys.\n",
    "As we will see later on, we will use `non_changed_test_set` for cross-validation.\n",
    "We also create our binary-encoded K-Nearest Neighbor (KNN) graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in our train and test sets...\n",
      "reading in our cross-validated test set...\n"
     ]
    }
   ],
   "source": [
    "print(f'reading in our train and test sets...')\n",
    "train_set = read_input('../train/cv_train/train_cvset_2.csv')\n",
    "test_set  = read_input('../test/rand_test/test_randset_2.csv')\n",
    "\n",
    "print(f'reading in our cross-validated test set...')\n",
    "non_changed_test_set = read_input('../test/cv_test/test_cvset_2.csv')\n",
    "\n",
    "knn_graph = np.zeros((train_set['width'], test_set['width']), dtype = int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "Once we have our inputs stored, we can begin applying our first hyper-parameter: pre-processing method.\n",
    "This notebook will demonstrate performance using the `logarithms` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing our data to use logarithms on our train and test sets...\n"
     ]
    }
   ],
   "source": [
    "print(f'pre-processing our data to use {selected_preprocess_method} on our train and test sets...')\n",
    "train_set['data'] = preprocess(train_set['data'], selected_preprocess_method)\n",
    "test_set['data']  = preprocess(test_set['data'], selected_preprocess_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "### Test Case Configuration\n",
    "\n",
    "### KNN Algorithm\n",
    "\n",
    "### Distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Out Recommendation Dataset\n",
    "Python's makes it easy to write out our recommendation data set for Kaggle submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Performance & Future Work\n",
    "Cross-validation helped assess the performance of our model locally.\n",
    "The following image shows the script-version's command line output when testing 3 cross-validated data sets.\n",
    "![assessment](./images/cross_validation.png)\n",
    "The test cases each ran with its own set of parameters to get a better idea of how well our KNN algorithm worked.\n",
    "The final step is to run the program with the best set of hyper-parameters to submit a Kaggle entry.\n",
    "![submission](./images/kaggle_ready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1]“NumPy Reference — NumPy v1.23 Manual,” numpy.org. https://numpy.org/doc/stable/reference/index.html#reference\n",
    "\n",
    "[2]“Running Python script on GPU.,” GeeksforGeeks, Aug. 21, 2019. https://www.geeksforgeeks.org/running-python-script-on-gpu/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
